{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.stats import kurtosis, skew\n",
    "import librosa.feature\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 1️⃣ Data Preprocessing\n",
    "# -----------------\n",
    "\n",
    "def load_labels(labels_csv):\n",
    "    \"\"\" \n",
    "    Load labels from CSV file into a dictionary \n",
    "    \n",
    "    \"\"\"\n",
    "    labels_df = pd.read_csv(labels_csv)\n",
    "    labels_dict = dict(zip(labels_df['id'], labels_df['pos_label']))\n",
    "    return labels_dict\n",
    "\n",
    "def get_audio_files_labels(folder, labels_dict, target_sr=16000, max_length=3200):\n",
    "    \"\"\" \n",
    "    Load all audio file paths and their corresponding labels \n",
    "    \n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder, file)\n",
    "            file_paths.append(file_path)\n",
    "            labels.append(labels_dict.get(file, 0))  # Default label 0 if not in dict\n",
    "    \n",
    "    return file_paths, labels\n",
    "\n",
    "def load_audio(file_path, target_sr=16000, max_length=3200):\n",
    "    \"\"\" \n",
    "    Load and normalize audio file \n",
    "\n",
    "    \"\"\"\n",
    "    signal, sr = librosa.load(file_path, sr=target_sr)\n",
    "    signal = librosa.util.fix_length(signal, size=max_length)\n",
    "    return signal\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for loading audio files and their labels\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, folder, labels_csv, target_sr=16000, max_length=3200):\n",
    "        self.labels_dict = load_labels(labels_csv)\n",
    "        self.file_paths, self.labels = get_audio_files_labels(folder, self.labels_dict, target_sr, max_length)\n",
    "        self.target_sr = target_sr\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signal = load_audio(self.file_paths[idx], self.target_sr, self.max_length)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(signal, dtype=torch.float32).unsqueeze(0), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# 2️⃣ Splitting Data into Training, Validation & Test\n",
    "# -----------------\n",
    "\n",
    "def create_dataloaders(folder, labels_csv, batch_size=32, train_split=0.7, val_split=0.15, test_split=0.15):\n",
    "    dataset = AudioDataset(folder, labels_csv)\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size = int(val_split * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# -----------------\n",
    "# 3️⃣ Model Architecture\n",
    "# -----------------\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=9, stride=1, padding=4)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 400, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# -----------------\n",
    "# 4️⃣ Training Pipeline with Early Stopping\n",
    "# -----------------\n",
    "\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    \"\"\"\n",
    "    Train model with early stopping based on validation loss\n",
    "\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\") \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered. Training stopped.\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# 5️⃣ Model Evaluation on Test Set\n",
    "# -----------------\n",
    "\n",
    "def evaluate_model(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"Test ROC AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/X_train\"  \n",
    "labels_csv = \"data/Y_train_ofTdMHi.csv\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.7374, Val Loss: 3.6637\n",
      "Epoch 2/20, Train Loss: 0.6563, Val Loss: 2.7142\n",
      "Epoch 3/20, Train Loss: 0.6502, Val Loss: 0.8929\n",
      "Epoch 4/20, Train Loss: 0.6462, Val Loss: 3.2708\n",
      "Epoch 5/20, Train Loss: 0.6375, Val Loss: 0.6824\n",
      "Epoch 6/20, Train Loss: 0.6254, Val Loss: 1.5675\n",
      "Epoch 7/20, Train Loss: 0.6207, Val Loss: 0.6834\n",
      "Epoch 8/20, Train Loss: 0.6080, Val Loss: 1.2065\n",
      "Epoch 9/20, Train Loss: 0.6059, Val Loss: 2.7145\n",
      "Epoch 10/20, Train Loss: 0.6014, Val Loss: 1.1666\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n",
      "\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n",
      "\u001b[1;32m---> 20\u001b[0m test_roc_auc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[1;32mIn[5], line 176\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(test_loader, model)\u001b[0m\n",
      "\u001b[0;32m    173\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;32m    174\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;32m--> 176\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m(all_labels, all_preds)\n",
      "\u001b[0;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest ROC AUC Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m roc_auc\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(data_folder, labels_csv, batch_size=32)\n",
    "\n",
    "# Initialize Model, Loss Function, and Optimizer\n",
    "model = CNN1D()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train Model\n",
    "trained_model = train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=20, patience=5)\n",
    "\n",
    "# Load the trained model\n",
    "model = CNN1D()\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC Score: 0.5095\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_roc_auc = evaluate_model(test_loader, model)# Evaluate on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the test data\n",
    "test_data_folder = \"data/X_test\"\n",
    "\n",
    "# Define the same parameters used in training\n",
    "target_sr = 16000\n",
    "max_length = 3200\n",
    "\n",
    "# Load the trained model\n",
    "model = CNN1D()\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to load and preprocess test audio files\n",
    "def load_test_audio(file_path, target_sr=16000, max_length=3200):\n",
    "    signal, sr = librosa.load(file_path, sr=target_sr)\n",
    "    signal = librosa.util.fix_length(signal, size=max_length)  \n",
    "    return torch.tensor(signal, dtype=torch.float32).unsqueeze(0).to(device)  \n",
    "\n",
    "# Get test file paths\n",
    "test_files = [f for f in os.listdir(test_data_folder) if f.endswith(\".wav\")]\n",
    "test_file_paths = [os.path.join(test_data_folder, f) for f in test_files]\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for file_path in test_file_paths:\n",
    "        input_tensor = load_test_audio(file_path, target_sr, max_length).unsqueeze(0)  \n",
    "        prob = model(input_tensor).squeeze().cpu().numpy()  \n",
    "        predictions.append((os.path.basename(file_path), prob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and save as CSV\n",
    "submission_df = pd.DataFrame(predictions, columns=[\"id\", \"prediction\"])\n",
    "submission_path = \"data/X_soumettre_CNN_1D.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score pour cette soumission est : 0,5033669433854705\n",
    "\n",
    "Ce modèle naive n'overfitte pas. Je vais tester d'autres hyperparamètres."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
